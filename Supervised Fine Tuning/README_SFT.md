<div align="center">
  
![Image](../img/logo.png)

</div><div align="left">
<h1>Supervised Fine Tuning
</h1>
</div>

æœ¬é¡¹ç›®åˆ†åˆ«åœ¨ **30wæ¡æ³•å¾‹é—®ç­”** [DISC-Law-SFT æ•°æ®é›†](https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT) å¾®è°ƒäº†[InternLM-chat-7B](https://huggingface.co/internlm/internlm-chat-7b)ã€[InternLM2-chat-7B](https://huggingface.co/internlm/internlm2-chat-7b)ä¸¤æ¬¾æ¨¡å‹ï¼Œå¹¶å…¬å¸ƒæƒé‡ã€‚å¦å¤–åœ¨[52kå•è½®é—®ç­”å’Œå¸¦æœ‰æ³•å¾‹ä¾æ®çš„æƒ…æ™¯é—®ç­”92kæ•°æ®é›†](https://github.com/LiuHC0428/LAW-GPT) ä¸Šå¾®è°ƒäº†[InternLM2-chat-20B](https://huggingface.co/internlm/internlm2-chat-20b)æ¨¡å‹ï¼Œå¹¶å…¬å¸ƒæƒé‡ã€‚

## ğŸ“„ Dataset Description

### 01 30wæ¡æ³•å¾‹é—®ç­” [DISC-Law-SFT æ•°æ®é›†](https://huggingface.co/datasets/ShengbinYue/DISC-Law-SFT) 

<table>
  <tr>
    <th>æ•°æ®é›†</th>
    <th>å¯¹åº”ä»»åŠ¡/æ¥æº</th>
    <th>æ ·æœ¬é‡</th>
    <th>å¯¹åº”æƒ…å¢ƒ</th>
  </tr>
  <tr>
    <td rowspan="10">DISC-Law-SFT-Pair</td>
    <td>å¸æ³•è¦ç´ æå–</td>
    <td>32K</td>
    <td rowspan="7">æ³•å¾‹ä¸“ä¸šäººå‘˜åŠ©æ‰‹</td>
  </tr>
  <tr>
    <td>å¸æ³•äº‹ä»¶æ£€æµ‹</td>
    <td>27K</td>
  </tr>
  <tr>
    <td>æ¡ˆä»¶åˆ†ç±»</td>
    <td>20K</td>
  </tr>
  <tr>
    <td>åˆ¤å†³é¢„æµ‹</td>
    <td>11K</td>
  </tr>
  <tr>
    <td>ç±»æ¡ˆåŒ¹é…</td>
    <td>8K</td>
  </tr>
  <tr>
    <td>å¸æ³•æ‘˜è¦</td>
    <td>9K</td>
  </tr>
  <tr>
    <td>èˆ†æƒ…æ‘˜è¦</td>
    <td>6K</td>
  </tr>
  <tr>
    <td>æ³•å¾‹é—®ç­”</td>
    <td>93K</td>
    <td>æ³•å¾‹å’¨è¯¢æœåŠ¡</td>
  </tr>
  <tr>
    <td>å¸æ³•é˜…è¯»ç†è§£</td>
    <td>38K</td>
    <td rowspan="2">æ³•å¾‹è€ƒè¯•åŠ©æ‰‹</td>
  </tr>
  <tr>
    <td>æ³•å¾‹è€ƒè¯•</td>
    <td>12K</td>
  </tr>
  <tr>
    <td rowspan="2">DISC-Law-SFT-Triplet</td>
    <td>åˆ¤å†³é¢„æµ‹</td>
    <td>16K</td>
    <td>æ³•å¾‹ä¸“ä¸šäººå‘˜åŠ©æ‰‹</td>
  </tr>
  <tr>
    <td>æ³•å¾‹é—®ç­”</td>
    <td>23K</td>
    <td>æ³•å¾‹å’¨è¯¢æœåŠ¡</td>
  </tr>
  <tr>
    <td>æ€»è®¡</td>
    <td colspan="3">295K</td>
  </tr>
</table>

### 02 52kå•è½®é—®ç­”å’Œå¸¦æœ‰æ³•å¾‹ä¾æ®çš„æƒ…æ™¯é—®ç­”92kæ•°æ®é›†[é“¾æ¥](https://github.com/LiuHC0428/LAW-GPT) 
çœŸå®çš„ä¸­æ–‡å¾‹å¸ˆç”¨æˆ·é—®ç­”æ•°æ®ï¼Œæ¥è‡ª[CrimeKgAssitant](https://github.com/liuhuanyong/CrimeKgAssitant)æ”¶é›†çš„200kæ¡æƒ…æ™¯å¯¹è¯æ•°æ®ã€‚ 

#### åˆ©ç”¨ChatGPTæ¸…æ´—CrimeKgAssitantæ•°æ®é›†å¾—åˆ°52kå•è½®é—®ç­”  


åˆ©ç”¨ChatGPTæ ¹æ®CrimeKgAssitantçš„é—®ç­”é‡æ–°ç”Ÿæˆï¼Œä½¿å¾—ç”Ÿæˆçš„å›ç­”æ¯”åŸå›ç­”æ›´è¯¦ç»†ï¼Œè¯­è¨€ç»„ç»‡æ›´è§„èŒƒã€‚
 
#### å¸¦æœ‰æ³•å¾‹ä¾æ®çš„æƒ…æ™¯é—®ç­”92k  

æ ¹æ®[ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹æ‰‹å†Œ](https://github.com/RanKKI/LawRefBook)ä¸Šæœ€æ ¸å¿ƒçš„9kæ³•å¾‹æ¡æ–‡ï¼Œåˆ©ç”¨ChatGPTè”æƒ³ç”Ÿæˆå…·ä½“çš„æƒ…æ™¯é—®ç­”ï¼Œä»è€Œä½¿å¾—ç”Ÿæˆçš„æ•°æ®é›†æœ‰å…·ä½“çš„æ³•å¾‹ä¾æ®ã€‚
æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
```json
"question": "åœ¨æŸå®¶å…¬å¸ä¸­ï¼Œä¸€åå‘˜å·¥å¯¹å¥³åŒäº‹å®æ–½äº†æ€§éªšæ‰°è¡Œä¸ºï¼Œå¥³åŒäº‹å‘å…¬å¸è¿›è¡Œä¸¾æŠ¥ï¼Œä½†å…¬å¸å´æ²¡æœ‰é‡‡å–å¿…è¦çš„æªæ–½æ¥åˆ¶æ­¢è¿™ç§è¡Œä¸ºã€‚\n\nå…¬å¸æœªé‡‡å–å¿…è¦æªæ–½é¢„é˜²å’Œåˆ¶æ­¢æ€§éªšæ‰°ï¼Œå¯¼è‡´å¥³åŒäº‹çš„æƒç›Šå—åˆ°ä¾µå®³ï¼Œè¯¥å…¬å¸æ˜¯å¦éœ€è¦æ‰¿æ‹…è´£ä»»ï¼Ÿ"
"answer": "æ ¹æ®ã€Šç¤¾ä¼šæ³•-å¦‡å¥³æƒç›Šä¿éšœæ³•ã€‹ç¬¬å…«åæ¡è§„å®šï¼Œâ€œå­¦æ ¡ã€ç”¨äººå•ä½è¿åæœ¬æ³•è§„å®šï¼Œæœªé‡‡å–å¿…è¦æªæ–½é¢„é˜²å’Œåˆ¶æ­¢æ€§éªšæ‰°ï¼Œé€ æˆå¦‡å¥³æƒç›Šå—åˆ°ä¾µå®³æˆ–è€…ç¤¾ä¼šå½±å“æ¶åŠ£çš„ï¼Œç”±ä¸Šçº§æœºå…³æˆ–è€…ä¸»ç®¡éƒ¨é—¨è´£ä»¤æ”¹æ­£ï¼›æ‹’ä¸æ”¹æ­£æˆ–è€…æƒ…èŠ‚ä¸¥é‡çš„ï¼Œä¾æ³•å¯¹ç›´æ¥è´Ÿè´£çš„ä¸»ç®¡äººå‘˜å’Œå…¶ä»–ç›´æ¥è´£ä»»äººå‘˜ç»™äºˆå¤„åˆ†ã€‚â€å› æ­¤ï¼Œè¯¥å…¬å¸å› ä¸ºæœªé‡‡å–å¿…è¦æªæ–½é¢„é˜²å’Œåˆ¶æ­¢æ€§éªšæ‰°è¡Œä¸ºï¼Œåº”è¯¥æ‰¿æ‹…è´£ä»»ï¼Œå¹¶ä¾æ³•æ¥å—ç›¸å…³çš„å¤„åˆ†ã€‚å¥³åŒäº‹å¯ä»¥å‘ä¸Šçº§æœºå…³æˆ–ä¸»ç®¡éƒ¨é—¨è¿›è¡Œç”³è¯‰ï¼Œè¦æ±‚è¯¥å…¬å¸è¢«è´£ä»¤æ”¹æ­£ï¼Œå¹¶å¯¹ç›¸å…³è´Ÿè´£äººå‘˜ç»™äºˆå¤„åˆ†ã€‚"
"reference": [
            "ç¤¾ä¼šæ³•-å¦‡å¥³æƒç›Šä¿éšœæ³•2022-10-30:    \"ç¬¬ä¸ƒåä¹æ¡ è¿åæœ¬æ³•ç¬¬äºŒåäºŒæ¡ç¬¬äºŒæ¬¾è§„å®šï¼Œæœªå±¥è¡ŒæŠ¥å‘Šä¹‰åŠ¡çš„ï¼Œä¾æ³•å¯¹ç›´æ¥è´Ÿè´£çš„ä¸»ç®¡äººå‘˜å’Œå…¶ä»–ç›´æ¥è´£ä»»äººå‘˜ç»™äºˆå¤„åˆ†ã€‚\",\n",
            "ç¤¾ä¼šæ³•-å¦‡å¥³æƒç›Šä¿éšœæ³•2022-10-30:    \"ç¬¬å…«åæ¡ è¿åæœ¬æ³•è§„å®šï¼Œå¯¹å¦‡å¥³å®æ–½æ€§éªšæ‰°çš„ï¼Œç”±å…¬å®‰æœºå…³ç»™äºˆæ‰¹è¯„æ•™è‚²æˆ–è€…å‡ºå…·å‘Šè¯«ä¹¦ï¼Œå¹¶ç”±æ‰€åœ¨å•ä½ä¾æ³•ç»™äºˆå¤„åˆ†ã€‚\",\n",
            "ç¤¾ä¼šæ³•-å¦‡å¥³æƒç›Šä¿éšœæ³•2022-10-30:    \"å­¦æ ¡ã€ç”¨äººå•ä½è¿åæœ¬æ³•è§„å®šï¼Œæœªé‡‡å–å¿…è¦æªæ–½é¢„é˜²å’Œåˆ¶æ­¢æ€§éªšæ‰°ï¼Œé€ æˆå¦‡å¥³æƒç›Šå—åˆ°ä¾µå®³æˆ–è€…ç¤¾ä¼šå½±å“æ¶åŠ£çš„ï¼Œç”±ä¸Šçº§æœºå…³æˆ–è€…ä¸»ç®¡éƒ¨é—¨è´£ä»¤æ”¹æ­£ï¼›æ‹’ä¸æ”¹æ­£æˆ–è€…æƒ…èŠ‚ä¸¥é‡çš„ï¼Œä¾æ³•å¯¹ç›´æ¥è´Ÿè´£çš„ä¸»ç®¡äººå‘˜å’Œå…¶ä»–ç›´æ¥è´£ä»»äººå‘˜ç»™äºˆå¤„åˆ†ã€‚\",\n",
            "ç¤¾ä¼šæ³•-å¦‡å¥³æƒç›Šä¿éšœæ³•2022-10-30:    \"ç¬¬å…«åä¸€æ¡ è¿åæœ¬æ³•ç¬¬äºŒåå…­æ¡è§„å®šï¼Œæœªå±¥è¡ŒæŠ¥å‘Šç­‰ä¹‰åŠ¡çš„ï¼Œä¾æ³•ç»™äºˆè­¦å‘Šã€è´£ä»¤åœä¸šæ•´é¡¿æˆ–è€…åŠé”€è¥ä¸šæ‰§ç…§ã€åŠé”€ç›¸å…³è®¸å¯è¯ï¼Œå¹¶å¤„ä¸€ä¸‡å…ƒä»¥ä¸Šäº”ä¸‡å…ƒä»¥ä¸‹ç½šæ¬¾ã€‚\",\n"
        ]
```

## ğŸ” Training Detail

### [<img src="../img/modelscope_logo.png" width="20px" /> Legal-Eagle-InternLM-chat-7B](https://www.modelscope.cn/models/wangzixinxinxin/Legal-Eagle-InternLM-chat-7B-Merged)  

```
accelerate launch  supervised_finetuning.py \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --use_peft True \
    --model_max_length 2048 \
    --num_train_epochs 2 \
    --learning_rate 2e-5 \
    --warmup_ratio 0.05 \
    --weight_decay 0.05 \
    --gradient_accumulation_steps 32 \
    --preprocessing_num_workers 4 \
    --target_modules all \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --fp16 \
    --device_map auto \
    --gradient_checkpointing True \
    --cache_dir ./cache

```
åœ¨ **4** å¼  **Nvidia GeForce RTX 4090** ä¸Šè¿è¡Œçº¦10hå®Œæˆè®­ç»ƒï¼Œlossæ”¶æ•›è‡³0.5å·¦å³ã€‚

---------------------------------- **Legal-Eagle-InternLM-chat-7B-loss** ---------------------------------- 
<div align="left">
    <img src="../img/Legal-Eagle-InternLM-chat-7B-loss.png" width="700">
</div>

### [<img src="../img/modelscope_logo.png" width="20px" /> Legal-Eagle-InternLM2-chat-7B](https://www.modelscope.cn/models/wangzixinxinxin/Legal-Eagle-InternLM2-chat-7B-Merged) 

```
accelerate launch  src/train_bash.py \
    --stage sft \
    --do_train \
    --finetuning_type lora \
    --lora_target all \
    --output_dir first_intern2_output \
    --overwrite_cache \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 400 \
    --learning_rate 5e-5 \
    --num_train_epochs 2.0 \
    --plot_loss \
    --fp16
```
åœ¨ **7** å¼  **NVIDIA GeForce RTX 4090** ä¸Šè¿è¡Œçº¦7hå®Œæˆè®­ç»ƒï¼Œlossæ”¶æ•›è‡³0.4å·¦å³ã€‚

---------------------------------- **Legal-Eagle-InternLM2-chat-7B-loss** ---------------------------------- 
<div align="left">
    <img src="../img/legal-Eagle-InternLM2-chat-7B-loss.png" width="750">
</div>

### [<img src="../img/modelscope_logo.png" width="20px" /> Legal-Eagle-InternLM2-chat-20B](https://www.modelscope.cn/models/wangzixinxinxin/Legal-Eagle-InternLM2-chat-20B-Adapter)

```

max_length = 2048
pack_to_max_length = True
batch_size = 4 
accumulative_counts = 16
dataloader_num_workers = 0
max_epochs = 2
optim_type = AdamW
lr = 1e-4
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip
warmup_ratio = 0.03
evaluation_freq = 1000
evaluation_inputs = ['è¯·é—®ç¦»å©šéœ€è¦å‡†å¤‡ä»€ä¹ˆææ–™ï¼Ÿ', 'é”€å”®é³„é±¼çš®åŒ…è¿æ³•å—ï¼Ÿ']

```
åœ¨ **1/2** å¼  **NVIDIA A100-SXM4-80GB** ä¸Šè¿è¡Œçº¦60h (2.5day) å®Œæˆè®­ç»ƒï¼Œlossæ”¶æ•›è‡³0.9å·¦å³ã€‚
